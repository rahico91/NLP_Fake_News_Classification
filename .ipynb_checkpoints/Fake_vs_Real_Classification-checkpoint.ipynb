{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classification\n",
    "\n",
    "A dataset from [Real or Fake](https://www.kaggle.com/rchitic17/real-or-fake) is provided containing different news articles.\n",
    "\n",
    "We want to build a  model that can classify if a given article is considered fake or not. We will use a subset of the data for training and the remaining for testing our model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "We separate the project in 3 steps:\n",
    "\n",
    "**Data Loading and Transforming:** Load the data and analyze it to obtain an accurate picture of it, its features, its values (and whether they are incomplete or wrong), its data types among others. We also do the required transformation steps.\n",
    "\n",
    "**Feature Engineering / Modeling:** Once we have the data, we create some features and then the modeling stage begins, making use of different models, we will hopefully produce a model that fits our expectations of performance. Once we have that model, a process of tuning it to the training data would be performed.\n",
    "\n",
    "**Results and Conclusions:** Finally, with our tuned model, we  predict against the test set, then we review those results against their actual values to determine the performance of the model, and finally, outline our conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rafaelhernandez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk as nl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score as metric_scorer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "nl.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Key Values\n",
    "\n",
    "The following values are used throught the code, this cell gives a central source where they can be managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Here we load the necessary data for training and testing, review its types and print its first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "test = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID        int64\n",
       "title    object\n",
       "text     object\n",
       "label    object\n",
       "X1       object\n",
       "X2       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday’s unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\\n\\nThe visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\\n\\nThe French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday’s march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\\n\\nAmong roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\\n\\nKerry spent Sunday at a business summit hosted by India’s prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi’s government will act to open the huge Indian market for more American businesses.\\n\\nIn a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as “quibbling a little bit.” He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\\n\\n“But that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,” he said.\\n\\n“And I don’t think the people of France have any doubts about America’s understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.”'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Since some rows have information spread accross different columns, we perform the required transformations in order to obtain the final text in a single column.\n",
    "\n",
    "#### X1 Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio  Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Black Hawk crashes off Florida  human remains ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                concat label\n",
       "192  Election Day: No Legal Pot In Ohio  Democrats ...  REAL\n",
       "308  Who rode it best? Jesse Jackson mounts up to f...  FAKE\n",
       "382  Black Hawk crashes off Florida  human remains ...  REAL\n",
       "660  Afghanistan: 19 die in air attacks on hospital...  REAL\n",
       "889  Al Qaeda rep says group directed Paris magazin...  REAL"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_X1 = train.loc[(train['X1'] == 'REAL') | (train['X1'] == 'FAKE')]\n",
    "fixed_X1 = pd.DataFrame(shifted_X1['title'].map(str) + ' ' + shifted_X1['text'].map(str) + ' ' + shifted_X1['label'].map(str), columns=[\"concat\"])\n",
    "fixed_X1['label'] = shifted_X1[\"X1\"]\n",
    "fixed_X1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X2 Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>Planned Parenthood’s lobbying effort  pay rais...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 concat label\n",
       "2184  Planned Parenthood’s lobbying effort  pay rais...  REAL\n",
       "3537  Chart Of The Day: Since 2009—–Recovery For The...  FAKE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_X2 = train.loc[(train['X2'] == 'REAL') | (train['X2'] == 'FAKE')]\n",
    "fixed_X2 = pd.DataFrame(shifted_X2['title'] + ' ' + shifted_X2['text'].map(str) + ' ' + shifted_X2['label'].map(str) + ' ' + shifted_X2['X1'].map(str), columns = ['concat'])\n",
    "fixed_X2['label'] = shifted_X2[\"X2\"]\n",
    "fixed_X2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear Daniel Greenfield...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy U....</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              concat label\n",
       "0  You Can Smell Hillary’s Fear Daniel Greenfield...  FAKE\n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
       "2  Kerry to go to Paris in gesture of sympathy U....  REAL\n",
       "3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
       "4  The Battle of New York: Why This Primary Matte...  REAL"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled = train.loc[(train['label'] == 'REAL') | (train['label'] == 'FAKE')]\n",
    "fixed_labeled = pd.DataFrame(labeled['title'].map(str) + ' ' + labeled['text'].map(str), columns = ['concat'])\n",
    "fixed_labeled['label'] = labeled[\"label\"]\n",
    "fixed_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenated Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cc = pd.concat([fixed_X1, fixed_X2, fixed_labeled], axis=0)\n",
    "train_cc[\"concat\"] = train_cc[\"concat\"]\n",
    "train_cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio  Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Black Hawk crashes off Florida  human remains ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                concat label\n",
       "192  Election Day: No Legal Pot In Ohio  Democrats ...  REAL\n",
       "308  Who rode it best? Jesse Jackson mounts up to f...  FAKE\n",
       "382  Black Hawk crashes off Florida  human remains ...  REAL\n",
       "660  Afghanistan: 19 die in air attacks on hospital...  REAL\n",
       "889  Al Qaeda rep says group directed Paris magazin...  REAL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenated Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>10498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>2439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              concat     ID\n",
       "0  September New Homes Sales Rise——-Back To 1992 ...  10498\n",
       "1  Why The Obamacare Doomsday Cult Can't Admit It...   2439\n",
       "2  Sanders, Cruz resist pressure after NY losses,...    864\n",
       "3  Surviving escaped prisoner likely fatigued and...   4128\n",
       "4  Clinton and Sanders neck and neck in Californi...    662"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cc = pd.DataFrame(test['title'].map(str) + ' ' + test['text'].map(str), columns = ['concat'])\n",
    "test_cc[\"concat\"] = test_cc[\"concat\"]\n",
    "test_cc['ID'] = test[\"ID\"]\n",
    "test_cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check target variable balance\n",
    "We review the distribution of values in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a29b11390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 7))\n",
    "target_count = (train_cc[\"label\"].value_counts() / len(train_cc)) * 100\n",
    "target_count.plot(kind=\"bar\", title=\"Target Distribution (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is perfect, therefore no resampling is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data\n",
    "We convert the different articles to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3999, 55777)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "final_train = count_vect.fit_transform(train_cc.concat)\n",
    "print(final_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized TF/TF-IDF Representation\n",
    "\n",
    "Now we transform the matrix of token counts to a normalized tf or tf-idf representation, where tf represents term frequency and tf-idf represents the frequency times the inverse document frequency, that way the importance/scale of certain repeated tokens throughout the text is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3999, 55777)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "final_train = tfidf_transformer.fit_transform(final_train)\n",
    "print(final_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "In order to test the performance of our feature engineering steps, we will create several initial baseline models, that way we will see how our efforts increase the models predictive power.\n",
    "\n",
    "### Train Function\n",
    "Here we define the train function which will be used with the different models, it performs a cross validation score on 80% of the training data and a final validation on the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, model, grid = None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "    \n",
    "    if grid:\n",
    "        model = RandomizedSearchCV(model, grid, cv=5, n_iter=10, refit=True, return_train_score=False, error_score=0.0, n_jobs=-1, random_state=SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "        scores = model.cv_results_[\"mean_test_score\"]\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Cross validation scores: \\n\" + str(scores))\n",
    "    print(\"Classification Report: \\n\" + str(classification_report(y_test, predictions)))\n",
    "    print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, predictions)))\n",
    "    print(\"Accuracy: \\n\" + str(metric_scorer(y_test, predictions)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.89375    0.8796875  0.8890625  0.8625     0.87636933]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.87      0.92      0.90       421\n",
      "        REAL       0.91      0.85      0.88       379\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       800\n",
      "   macro avg       0.89      0.89      0.89       800\n",
      "weighted avg       0.89      0.89      0.89       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[388  33]\n",
      " [ 56 323]]\n",
      "Accuracy: \n",
      "0.88875\n"
     ]
    }
   ],
   "source": [
    "X = final_train\n",
    "y = train_cc['label']\n",
    "rf = train(X, y, RandomForestClassifier(n_estimators=100, random_state=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.725      0.721875   0.7546875  0.7015625  0.72143975]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.98      0.54      0.70       421\n",
      "        REAL       0.66      0.99      0.79       379\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       800\n",
      "   macro avg       0.82      0.77      0.75       800\n",
      "weighted avg       0.83      0.76      0.74       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[229 192]\n",
      " [  4 375]]\n",
      "Accuracy: \n",
      "0.755\n"
     ]
    }
   ],
   "source": [
    "nb = train(X, y, MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.8921875  0.8921875  0.89375    0.9046875  0.89514867]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.91      0.93      0.92       421\n",
      "        REAL       0.92      0.89      0.91       379\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       800\n",
      "   macro avg       0.91      0.91      0.91       800\n",
      "weighted avg       0.91      0.91      0.91       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[391  30]\n",
      " [ 40 339]]\n",
      "Accuracy: \n",
      "0.9125\n"
     ]
    }
   ],
   "source": [
    "lrg = train(X, y, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.896875   0.9109375  0.8890625  0.90625    0.90140845]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.88      0.95      0.92       421\n",
      "        REAL       0.94      0.86      0.90       379\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       800\n",
      "   macro avg       0.91      0.91      0.91       800\n",
      "weighted avg       0.91      0.91      0.91       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[402  19]\n",
      " [ 54 325]]\n",
      "Accuracy: \n",
      "0.90875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm = train(X, y, SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.934375   0.946875   0.9296875  0.925      0.93114241]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.93      0.94      0.93       421\n",
      "        REAL       0.93      0.92      0.92       379\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       800\n",
      "   macro avg       0.93      0.93      0.93       800\n",
      "weighted avg       0.93      0.93      0.93       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[394  27]\n",
      " [ 30 349]]\n",
      "Accuracy: \n",
      "0.92875\n"
     ]
    }
   ],
   "source": [
    "pac = train(X, y, PassiveAggressiveClassifier(max_iter=1000, random_state=SEED, tol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio  Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Black Hawk crashes off Florida  human remains ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                concat label\n",
       "192  Election Day: No Legal Pot In Ohio  Democrats ...  REAL\n",
       "308  Who rode it best? Jesse Jackson mounts up to f...  FAKE\n",
       "382  Black Hawk crashes off Florida  human remains ...  REAL\n",
       "660  Afghanistan: 19 die in air attacks on hospital...  REAL\n",
       "889  Al Qaeda rep says group directed Paris magazin...  REAL"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "# https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "# https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans each doc string by doing the following: \n",
    "\n",
    "-Removing punctuation and other non alphabetical characters ( using re.sub(\"[^a-zA-Z]\") )\n",
    "\n",
    "-Convert to Lower case and split string into words (tokenization)\n",
    "\n",
    "-Removes stop words (most frequent words)\n",
    "\n",
    "-Doing Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_train = train_cc.copy()\n",
    "# words = stopwords.words(\"english\")\n",
    "# if (stem=='S'):  # Choosing between Stemming ('S') and Lemmatization ('L')\n",
    "#     stemmer=PorterStemmer()\n",
    "#     final_train['concat'] = final_train['concat'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "# else: \n",
    "#     lemma=WordNetLemmatizer()\n",
    "#     final_train['concat'] = final_train['concat'].apply(lambda x:' '.join([lemmatizer.lemmatize(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "# final_train.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_train = train_cc.copy()\n",
    "# lemmatizer=WordNetLemmatizer()\n",
    "# words = stopwords.words(\"english\")\n",
    "# final_train['concat'] = final_train['concat'].apply(lambda x:' '.join([lemmatizer.lemmatize(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "# final_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>elect day no legal pot in ohio democrat lose i...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>who rode best jess jackson mount fight pipelin...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>black hawk crash florida human remain found cn...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>afghanistan die air attack hospit u s investig...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>al qaeda rep say group direct pari magazin att...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                concat label\n",
       "192  elect day no legal pot in ohio democrat lose i...  REAL\n",
       "308  who rode best jess jackson mount fight pipelin...  FAKE\n",
       "382  black hawk crash florida human remain found cn...  REAL\n",
       "660  afghanistan die air attack hospit u s investig...  REAL\n",
       "889  al qaeda rep say group direct pari magazin att...  REAL"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train = train_cc.copy()\n",
    "words = stopwords.words(\"english\")\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(language='english') #try both Porter and Snowball\n",
    "final_train['concat'] = final_train['concat'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "final_train.head()\n",
    "                                                    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.940625   0.9421875  0.9375     0.9453125  0.94522692]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.95      0.96      0.95       421\n",
      "        REAL       0.95      0.95      0.95       379\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       800\n",
      "   macro avg       0.95      0.95      0.95       800\n",
      "weighted avg       0.95      0.95      0.95       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[403  18]\n",
      " [ 20 359]]\n",
      "Accuracy: \n",
      "0.9525\n",
      "Best Score: \n",
      "0.9374804626445764\n",
      "Best Params: \n",
      "{'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "X = final_train['concat']\n",
    "y = final_train['label']\n",
    "\n",
    "parameters = {'vect__analyzer'      : ('word', 'char'),\n",
    "              'vect__ngram_range'   : [(1,1),(1,3),(1,5)],\n",
    "              'vect__stop_words'    : ('english', None),\n",
    "              'vect__strip_accents' : ('unicode', None),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))),\n",
    "#     ('rf', RandomForestClassifier()),\n",
    "#     ('nb', MultinomialNB()),\n",
    "#     ('lgr' LogisticRegression()),\n",
    "    ('pac', PassiveAggressiveClassifier(max_iter=1000, random_state=SEED, tol=1e-3))#,\n",
    "#     ('svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=SEED))\n",
    "])\n",
    "\n",
    "pac_clf = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "pac_pipeline = train(X, y, pac_clf)\n",
    "\n",
    "print(\"Best Grid CV Score: \\n\" + str(pac_clf.best_score_))\n",
    "print(\"Best Grid Params: \\n\" + str(pac_clf.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/rafaelhernandez/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: \n",
      "[0.921875  0.925     0.91875   0.9375    0.9170579]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.93      0.96      0.94       421\n",
      "        REAL       0.95      0.92      0.94       379\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       800\n",
      "   macro avg       0.94      0.94      0.94       800\n",
      "weighted avg       0.94      0.94      0.94       800\n",
      "\n",
      "Confusion Matrix: \n",
      "[[403  18]\n",
      " [ 30 349]]\n",
      "Accuracy: \n",
      "0.94\n",
      "Best Score: \n",
      "0.9221631759924976\n",
      "Best Score: \n",
      "{'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}\n",
      "Best Score: \n",
      "{'mean_fit_time': array([ 1.24840697,  1.24953206,  1.31556861,  1.40960161,  5.90614446,\n",
      "        5.87282538,  6.45441596,  6.4671483 , 14.00030565, 12.98634529,\n",
      "       15.57404399, 14.17756828, 19.92997034, 22.3259275 , 27.52715373,\n",
      "       26.19625433, 25.79879363, 35.43764377, 41.46618764, 32.15281169,\n",
      "        2.07599099,  2.23843098,  2.18554441,  2.0379823 ,  7.62202565,\n",
      "        8.72406944,  7.46988161,  7.44889927, 16.93092974, 14.46484296,\n",
      "       13.62942632, 13.8514723 , 22.62944174, 22.63998477, 23.33363692,\n",
      "       23.38817374, 41.89337722, 43.0354627 , 39.33237465, 34.25332483,\n",
      "        6.52338958,  6.51222436,  6.69846408,  8.14972067, 15.89430197,\n",
      "       15.26073281, 15.21516697, 14.63574568, 19.71835899, 22.90761129,\n",
      "       23.43359566, 21.37297201, 25.87429746, 26.57915298, 26.66726677,\n",
      "       26.88738171, 44.54121184, 39.61722898, 38.51100405, 41.81002911]), 'std_fit_time': array([0.02016687, 0.02383568, 0.08323029, 0.00819088, 0.04696925,\n",
      "       0.01453805, 0.16530486, 0.0144085 , 0.2733754 , 0.81215603,\n",
      "       0.35432104, 0.37314581, 0.47830288, 1.57591359, 0.9238757 ,\n",
      "       1.55405331, 0.82477081, 6.18286185, 5.32951429, 0.67242226,\n",
      "       0.13615239, 0.01897757, 0.03280295, 0.02246475, 0.81134746,\n",
      "       0.05417635, 0.07678736, 0.064826  , 0.41753414, 1.1643664 ,\n",
      "       0.19931228, 0.16501773, 0.04916951, 0.26014607, 0.8366213 ,\n",
      "       0.24890035, 0.25946749, 0.61970728, 3.32048872, 0.14363481,\n",
      "       0.04563311, 0.11243914, 0.50999493, 0.67012347, 0.14941544,\n",
      "       0.38562559, 0.23781073, 0.19543063, 0.94464787, 1.65227546,\n",
      "       0.47964103, 0.60031342, 0.88382054, 0.09731429, 0.37574708,\n",
      "       0.13501871, 0.76198367, 3.59869949, 2.03524719, 0.63287542]), 'mean_score_time': array([ 0.61554337,  0.64808758,  0.71723938,  0.72161905,  1.23675259,\n",
      "        1.29388706,  1.444882  ,  1.37177134,  2.21586363,  2.56232667,\n",
      "        2.84867899,  2.0668207 ,  3.13412229,  3.05297343,  2.95563634,\n",
      "        2.70711605,  2.87144065,  3.32273293,  3.79668951,  3.52367862,\n",
      "        1.15188503,  1.17071597,  1.2155242 ,  1.18332632,  5.28346864,\n",
      "        4.17456492,  4.48845061,  4.48260395,  9.57032132,  8.69762707,\n",
      "        8.48197865, 10.36378272, 12.83867606, 12.78505858, 12.79714966,\n",
      "       13.42805576, 22.54529103, 20.25306296, 20.30789344, 20.30522362,\n",
      "        3.57897751,  3.45538807,  3.88001164,  4.36828963,  7.94435   ,\n",
      "        8.22254165,  8.19527817,  8.61725346, 12.06841246, 12.34792376,\n",
      "       12.31338271, 10.88767266, 17.20097915, 17.55479264, 17.0163149 ,\n",
      "       16.59950233, 25.59456038, 19.93698899, 18.07561588, 18.56917119]), 'std_score_time': array([0.00698998, 0.02501753, 0.00470919, 0.01333736, 0.0175936 ,\n",
      "       0.03830761, 0.12178614, 0.03700756, 0.1625539 , 0.36480542,\n",
      "       0.71272833, 0.13012545, 0.24079549, 0.078665  , 0.37858066,\n",
      "       0.17635373, 0.18233728, 0.13687598, 0.40720382, 0.27314484,\n",
      "       0.01749323, 0.04705173, 0.04507941, 0.01777724, 0.69085541,\n",
      "       0.19552393, 0.11431208, 0.07276338, 0.33082835, 1.18618038,\n",
      "       0.55822834, 0.42890305, 0.20132408, 0.26466615, 0.36957285,\n",
      "       0.58812694, 0.56446003, 1.42629943, 0.82834952, 0.47275118,\n",
      "       0.06900182, 0.00657724, 0.64505364, 0.36404783, 0.09560779,\n",
      "       0.18408094, 0.17867614, 0.33840526, 0.90353437, 0.80931189,\n",
      "       1.21915023, 0.25057653, 0.53254301, 0.48340379, 1.42689535,\n",
      "       0.44561102, 0.73595174, 3.7677563 , 0.60508738, 0.68529461]), 'param_vect__analyzer': masked_array(data=['word', 'word', 'word', 'word', 'word', 'word', 'word',\n",
      "                   'word', 'word', 'word', 'word', 'word', 'word', 'word',\n",
      "                   'word', 'word', 'word', 'word', 'word', 'word', 'char',\n",
      "                   'char', 'char', 'char', 'char', 'char', 'char', 'char',\n",
      "                   'char', 'char', 'char', 'char', 'char', 'char', 'char',\n",
      "                   'char', 'char', 'char', 'char', 'char', 'char_wb',\n",
      "                   'char_wb', 'char_wb', 'char_wb', 'char_wb', 'char_wb',\n",
      "                   'char_wb', 'char_wb', 'char_wb', 'char_wb', 'char_wb',\n",
      "                   'char_wb', 'char_wb', 'char_wb', 'char_wb', 'char_wb',\n",
      "                   'char_wb', 'char_wb', 'char_wb', 'char_wb'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 2), (1, 2),\n",
      "                   (1, 2), (1, 3), (1, 3), (1, 3), (1, 3), (1, 4), (1, 4),\n",
      "                   (1, 4), (1, 4), (1, 5), (1, 5), (1, 5), (1, 5), (1, 1),\n",
      "                   (1, 1), (1, 1), (1, 1), (1, 2), (1, 2), (1, 2), (1, 2),\n",
      "                   (1, 3), (1, 3), (1, 3), (1, 3), (1, 4), (1, 4), (1, 4),\n",
      "                   (1, 4), (1, 5), (1, 5), (1, 5), (1, 5), (1, 1), (1, 1),\n",
      "                   (1, 1), (1, 1), (1, 2), (1, 2), (1, 2), (1, 2), (1, 3),\n",
      "                   (1, 3), (1, 3), (1, 3), (1, 4), (1, 4), (1, 4), (1, 4),\n",
      "                   (1, 5), (1, 5), (1, 5), (1, 5)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_vect__stop_words': masked_array(data=['english', 'english', None, None, 'english', 'english',\n",
      "                   None, None, 'english', 'english', None, None,\n",
      "                   'english', 'english', None, None, 'english', 'english',\n",
      "                   None, None, 'english', 'english', None, None,\n",
      "                   'english', 'english', None, None, 'english', 'english',\n",
      "                   None, None, 'english', 'english', None, None,\n",
      "                   'english', 'english', None, None, 'english', 'english',\n",
      "                   None, None, 'english', 'english', None, None,\n",
      "                   'english', 'english', None, None, 'english', 'english',\n",
      "                   None, None, 'english', 'english', None, None],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_vect__strip_accents': masked_array(data=['unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None,\n",
      "                   'unicode', None, 'unicode', None, 'unicode', None],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'vect__analyzer': 'word', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'word', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 4), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 5), 'vect__stop_words': 'english', 'vect__strip_accents': None}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': 'unicode'}, {'vect__analyzer': 'char_wb', 'vect__ngram_range': (1, 5), 'vect__stop_words': None, 'vect__strip_accents': None}], 'split0_test_score': array([0.91565136, 0.91565136, 0.92408622, 0.92408622, 0.9212746 ,\n",
      "       0.9212746 , 0.92408622, 0.92408622, 0.92033739, 0.92033739,\n",
      "       0.92783505, 0.92783505, 0.91940019, 0.91940019, 0.92783505,\n",
      "       0.92783505, 0.91752577, 0.91752577, 0.92877226, 0.92877226,\n",
      "       0.55014058, 0.55014058, 0.55014058, 0.55014058, 0.77881912,\n",
      "       0.77881912, 0.77881912, 0.77881912, 0.91846298, 0.91846298,\n",
      "       0.91846298, 0.91846298, 0.92596064, 0.92596064, 0.92596064,\n",
      "       0.92596064, 0.92783505, 0.92783505, 0.92783505, 0.92783505,\n",
      "       0.54170572, 0.54170572, 0.54170572, 0.54170572, 0.78819119,\n",
      "       0.78819119, 0.78819119, 0.78819119, 0.91658857, 0.91658857,\n",
      "       0.91658857, 0.91658857, 0.92408622, 0.92408622, 0.92408622,\n",
      "       0.92408622, 0.92502343, 0.92502343, 0.92502343, 0.92502343]), 'split1_test_score': array([0.90900563, 0.90900563, 0.90900563, 0.90900563, 0.90243902,\n",
      "       0.90243902, 0.90806754, 0.90806754, 0.90337711, 0.90337711,\n",
      "       0.90900563, 0.90900563, 0.90806754, 0.90806754, 0.91275797,\n",
      "       0.91275797, 0.90712946, 0.90712946, 0.91369606, 0.91369606,\n",
      "       0.5750469 , 0.5750469 , 0.5750469 , 0.5750469 , 0.86116323,\n",
      "       0.86116323, 0.86116323, 0.86116323, 0.89118199, 0.89118199,\n",
      "       0.89118199, 0.89118199, 0.89868668, 0.89868668, 0.89868668,\n",
      "       0.89868668, 0.89774859, 0.89774859, 0.89774859, 0.89774859,\n",
      "       0.56003752, 0.56003752, 0.56003752, 0.56003752, 0.8564728 ,\n",
      "       0.8564728 , 0.8564728 , 0.8564728 , 0.88555347, 0.88555347,\n",
      "       0.88555347, 0.88555347, 0.89774859, 0.89774859, 0.89774859,\n",
      "       0.89774859, 0.89774859, 0.89774859, 0.89774859, 0.89774859]), 'split2_test_score': array([0.91744841, 0.91744841, 0.92307692, 0.92307692, 0.92776735,\n",
      "       0.92776735, 0.92589118, 0.92589118, 0.92776735, 0.92776735,\n",
      "       0.92401501, 0.92401501, 0.92682927, 0.92682927, 0.92401501,\n",
      "       0.92401501, 0.92776735, 0.92776735, 0.92401501, 0.92401501,\n",
      "       0.60881801, 0.60881801, 0.60881801, 0.60881801, 0.8217636 ,\n",
      "       0.8217636 , 0.8217636 , 0.8217636 , 0.90994371, 0.90994371,\n",
      "       0.90994371, 0.90994371, 0.91744841, 0.91744841, 0.91744841,\n",
      "       0.91744841, 0.92589118, 0.92589118, 0.92589118, 0.92589118,\n",
      "       0.60881801, 0.60881801, 0.60881801, 0.60881801, 0.83020638,\n",
      "       0.83020638, 0.83020638, 0.83020638, 0.91275797, 0.91275797,\n",
      "       0.91275797, 0.91275797, 0.9249531 , 0.9249531 , 0.9249531 ,\n",
      "       0.9249531 , 0.92870544, 0.92870544, 0.92870544, 0.92870544]), 'mean_test_score': array([0.91403564, 0.91403564, 0.9187246 , 0.9187246 , 0.91716161,\n",
      "       0.91716161, 0.9193498 , 0.9193498 , 0.91716161, 0.91716161,\n",
      "       0.92028759, 0.92028759, 0.91809941, 0.91809941, 0.92153798,\n",
      "       0.92153798, 0.91747421, 0.91747421, 0.92216318, 0.92216318,\n",
      "       0.57799312, 0.57799312, 0.57799312, 0.57799312, 0.82056893,\n",
      "       0.82056893, 0.82056893, 0.82056893, 0.90653329, 0.90653329,\n",
      "       0.90653329, 0.90653329, 0.91403564, 0.91403564, 0.91403564,\n",
      "       0.91403564, 0.91716161, 0.91716161, 0.91716161, 0.91716161,\n",
      "       0.57017818, 0.57017818, 0.57017818, 0.57017818, 0.8249453 ,\n",
      "       0.8249453 , 0.8249453 , 0.8249453 , 0.9049703 , 0.9049703 ,\n",
      "       0.9049703 , 0.9049703 , 0.91559862, 0.91559862, 0.91559862,\n",
      "       0.91559862, 0.91716161, 0.91716161, 0.91716161, 0.91716161]), 'std_test_score': array([0.00363082, 0.00363082, 0.00688309, 0.00688309, 0.01074028,\n",
      "       0.01074028, 0.00800986, 0.00800986, 0.01020607, 0.01020607,\n",
      "       0.00812675, 0.00812675, 0.00771333, 0.00771333, 0.0063999 ,\n",
      "       0.0063999 , 0.00842415, 0.00842415, 0.00629299, 0.00629299,\n",
      "       0.0240472 , 0.0240472 , 0.0240472 , 0.0240472 , 0.03363007,\n",
      "       0.03363007, 0.03363007, 0.03363007, 0.01139624, 0.01139624,\n",
      "       0.01139624, 0.01139624, 0.01139378, 0.01139378, 0.01139378,\n",
      "       0.01139378, 0.01374679, 0.01374679, 0.01374679, 0.01374679,\n",
      "       0.02832289, 0.02832289, 0.02832289, 0.02832289, 0.02812503,\n",
      "       0.02812503, 0.02812503, 0.02812503, 0.01381536, 0.01381536,\n",
      "       0.01381536, 0.01381536, 0.01262388, 0.01262388, 0.01262388,\n",
      "       0.01262388, 0.01380595, 0.01380595, 0.01380595, 0.01380595]), 'rank_test_score': array([31, 31,  9,  9, 15, 15,  7,  7, 15, 15,  5,  5, 11, 11,  3,  3, 13,\n",
      "       13,  1,  1, 53, 53, 53, 53, 49, 49, 49, 49, 37, 37, 37, 37, 31, 31,\n",
      "       31, 31, 15, 15, 15, 15, 57, 57, 57, 57, 45, 45, 45, 45, 41, 41, 41,\n",
      "       41, 27, 27, 27, 27, 15, 15, 15, 15], dtype=int32), 'split0_train_score': array([0.97185741, 0.97185741, 0.97560976, 0.97560976, 0.98452158,\n",
      "       0.98452158, 0.98639775, 0.98639775, 0.98545966, 0.98545966,\n",
      "       0.98874296, 0.98874296, 0.98592871, 0.98592871, 0.98827392,\n",
      "       0.98827392, 0.98639775, 0.98639775, 0.98921201, 0.98921201,\n",
      "       0.53424015, 0.53424015, 0.53424015, 0.53424015, 0.815197  ,\n",
      "       0.815197  , 0.815197  , 0.815197  , 0.94699812, 0.94699812,\n",
      "       0.94699812, 0.94699812, 0.97091932, 0.97091932, 0.97091932,\n",
      "       0.97091932, 0.97701689, 0.97701689, 0.97701689, 0.97701689,\n",
      "       0.53095685, 0.53095685, 0.53095685, 0.53095685, 0.82457786,\n",
      "       0.82457786, 0.82457786, 0.82457786, 0.94418386, 0.94418386,\n",
      "       0.94418386, 0.94418386, 0.96482176, 0.96482176, 0.96482176,\n",
      "       0.96482176, 0.96857411, 0.96857411, 0.96857411, 0.96857411]), 'split1_train_score': array([0.9737459 , 0.9737459 , 0.97796531, 0.97796531, 0.98452883,\n",
      "       0.98452883, 0.98921707, 0.98921707, 0.98499766, 0.98499766,\n",
      "       0.99109236, 0.99109236, 0.9859353 , 0.9859353 , 0.99109236,\n",
      "       0.99109236, 0.9859353 , 0.9859353 , 0.99203   , 0.99203   ,\n",
      "       0.5827473 , 0.5827473 , 0.5827473 , 0.5827473 , 0.86826067,\n",
      "       0.86826067, 0.86826067, 0.86826067, 0.95218003, 0.95218003,\n",
      "       0.95218003, 0.95218003, 0.9676512 , 0.9676512 , 0.9676512 ,\n",
      "       0.9676512 , 0.97890295, 0.97890295, 0.97890295, 0.97890295,\n",
      "       0.56118143, 0.56118143, 0.56118143, 0.56118143, 0.86966714,\n",
      "       0.86966714, 0.86966714, 0.86966714, 0.94608533, 0.94608533,\n",
      "       0.94608533, 0.94608533, 0.96249414, 0.96249414, 0.96249414,\n",
      "       0.96249414, 0.9676512 , 0.9676512 , 0.9676512 , 0.9676512 ]), 'split2_train_score': array([0.97515237, 0.97515237, 0.97609001, 0.97609001, 0.9859353 ,\n",
      "       0.9859353 , 0.98499766, 0.98499766, 0.98499766, 0.98499766,\n",
      "       0.98546648, 0.98546648, 0.98546648, 0.98546648, 0.98546648,\n",
      "       0.98546648, 0.98499766, 0.98499766, 0.9859353 , 0.9859353 ,\n",
      "       0.60056259, 0.60056259, 0.60056259, 0.60056259, 0.87388654,\n",
      "       0.87388654, 0.87388654, 0.87388654, 0.95311767, 0.95311767,\n",
      "       0.95311767, 0.95311767, 0.9676512 , 0.9676512 , 0.9676512 ,\n",
      "       0.9676512 , 0.97562119, 0.97562119, 0.97562119, 0.97562119,\n",
      "       0.59587436, 0.59587436, 0.59587436, 0.59587436, 0.87576184,\n",
      "       0.87576184, 0.87576184, 0.87576184, 0.94842944, 0.94842944,\n",
      "       0.94842944, 0.94842944, 0.96483826, 0.96483826, 0.96483826,\n",
      "       0.96483826, 0.96812002, 0.96812002, 0.96812002, 0.96812002]), 'mean_train_score': array([0.97358523, 0.97358523, 0.97655503, 0.97655503, 0.98499524,\n",
      "       0.98499524, 0.98687082, 0.98687082, 0.98515166, 0.98515166,\n",
      "       0.98843393, 0.98843393, 0.98577683, 0.98577683, 0.98827759,\n",
      "       0.98827759, 0.9857769 , 0.9857769 , 0.9890591 , 0.9890591 ,\n",
      "       0.57251668, 0.57251668, 0.57251668, 0.57251668, 0.85244807,\n",
      "       0.85244807, 0.85244807, 0.85244807, 0.95076528, 0.95076528,\n",
      "       0.95076528, 0.95076528, 0.96874057, 0.96874057, 0.96874057,\n",
      "       0.96874057, 0.97718034, 0.97718034, 0.97718034, 0.97718034,\n",
      "       0.56267088, 0.56267088, 0.56267088, 0.56267088, 0.85666894,\n",
      "       0.85666894, 0.85666894, 0.85666894, 0.94623288, 0.94623288,\n",
      "       0.94623288, 0.94623288, 0.96405139, 0.96405139, 0.96405139,\n",
      "       0.96405139, 0.96811511, 0.96811511, 0.96811511, 0.96811511]), 'std_train_score': array([0.00134995, 0.00134995, 0.00101631, 0.00101631, 0.00066473,\n",
      "       0.00066473, 0.00175475, 0.00175475, 0.00021779, 0.00021779,\n",
      "       0.00230713, 0.00230713, 0.00021947, 0.00021947, 0.00229676,\n",
      "       0.00229676, 0.00058246, 0.00058246, 0.0024905 , 0.0024905 ,\n",
      "       0.02802577, 0.02802577, 0.02802577, 0.02802577, 0.02644043,\n",
      "       0.02644043, 0.02644043, 0.02644043, 0.00269114, 0.00269114,\n",
      "       0.00269114, 0.00269114, 0.00154061, 0.00154061, 0.00154061,\n",
      "       0.00154061, 0.00134475, 0.00134475, 0.00134475, 0.00134475,\n",
      "       0.02652338, 0.02652338, 0.02652338, 0.02652338, 0.02282783,\n",
      "       0.02282783, 0.02282783, 0.02282783, 0.00173639, 0.00173639,\n",
      "       0.00173639, 0.00173639, 0.00110116, 0.00110116, 0.00110116,\n",
      "       0.00110116, 0.00037679, 0.00037679, 0.00037679, 0.00037679])}\n"
     ]
    }
   ],
   "source": [
    "X = final_train['concat']\n",
    "y = final_train['label']\n",
    "\n",
    "parameters = {'vect__analyzer'      : ('word', 'char'),\n",
    "              'vect__ngram_range'   : [(1,1),(1,3),(1,5)],\n",
    "              'vect__stop_words'    : ('english', None),\n",
    "              'vect__strip_accents' : ('unicode', None),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 3))),\n",
    "#     ('rf', RandomForestClassifier()),\n",
    "#     ('nb', MultinomialNB()),\n",
    "#     ('lgr' LogisticRegression()),\n",
    "#     ('pac', PassiveAggressiveClassifier(max_iter=1000, random_state=SEED, tol=1e-3))#,\n",
    "    ('sdg', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=SEED))\n",
    "])\n",
    "\n",
    "\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "pipeline = train(X, y, gs_clf)\n",
    "\n",
    "print(\"Best Score: \\n\" + str(gs_clf.best_score_))\n",
    "print(\"Best Score: \\n\" + str(gs_clf.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested different \"ngram_range\" configurations with (1, 2) being the optimal solution both on accuray and running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing on Passive Agressive Classifier -> Accuracy: 0.945\n",
    "\n",
    "PorterStemmer on Passive Agressive Classifier -> Accuracy: 0.94625\n",
    "\n",
    "SnowballStemmer on Passive Agressive Classifier -> Accuracy: 0.94875\n",
    "\n",
    "Lemmatizing & SnowballStemmer on Passive Agressive Classifier -> Accuracy: 0.94625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = test_cc.copy()\n",
    "final_test['concat'] = final_test['concat'].apply(lambda x:' '.join([lemmatizer.lemmatize(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "final_test[\"predictions\"] = pipeline.predict(final_test[\"concat\"])\n",
    "final_test.columns = [\"concat\", \"News_id\", \"prediction\"]\n",
    "final_test[[\"News_id\", \"prediction\"]].to_csv(\"predictions.csv\", index=False)\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The classification report obtained from our final model shows its precision (how often the predictions are correct) and the recall (how many of the total observations in the set are correctly classified), also its f1-score (harmonic average of both). The weighted average for all of them stands at 93%, it also has an accuracy of 92.75% which means that it can classify which articles are fake with great efficacy.\n",
    "\n",
    "This information is extremely useful to multiple actors, including social networks and end consumers since it can help them differentiate between real and fake stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
